{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle_environments in c:\\users\\antho\\anaconda3\\envs\\tensorflow\\lib\\site-packages (1.7.10)\n",
      "Requirement already satisfied: cpprb in c:\\users\\antho\\anaconda3\\envs\\tensorflow\\lib\\site-packages (9.4.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\antho\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from cpprb) (1.19.4)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in c:\\users\\antho\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from kaggle_environments) (3.2.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\antho\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from jsonschema>=3.0.1->kaggle_environments) (1.15.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\antho\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from jsonschema>=3.0.1->kaggle_environments) (0.17.3)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\antho\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from jsonschema>=3.0.1->kaggle_environments) (3.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\antho\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from jsonschema>=3.0.1->kaggle_environments) (49.6.0.post20201009)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\antho\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from jsonschema>=3.0.1->kaggle_environments) (20.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\antho\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from importlib-metadata->jsonschema>=3.0.1->kaggle_environments) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\antho\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from importlib-metadata->jsonschema>=3.0.1->kaggle_environments) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U kaggle_environments cpprb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment football failed: No module named 'gfootball'\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cpprb # Replay Buffer Library: https://ymd_h.gitlab.io/cpprb/\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\n",
    "from kaggle_environments import make\n",
    "\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global config\n",
    "#RIGHT = 0\n",
    "#GO = 1\n",
    "#LEFT = 2\n",
    "\n",
    "GOOSE = -1\n",
    "FOOD = 1\n",
    "\n",
    "act_shape = 4\n",
    "\n",
    "WIDTH = 11\n",
    "HEIGHT = 7\n",
    "\n",
    "xc = WIDTH//2 + 1\n",
    "yc = HEIGHT//2 + 1\n",
    "\n",
    "EAST_idx  = (xc+1,yc  )\n",
    "NORTH_idx = (xc  ,yc-1)\n",
    "WEST_idx  = (xc-1,yc  )\n",
    "SOUTH_idx = (xc  ,yc+1)\n",
    "\n",
    "AROUND = ([xc+1,xc  ,xc-1,xc  ],\n",
    "          [yc  ,yc-1,yc  ,yc+1])\n",
    "\n",
    "code2dir = {0:'EAST', 1:'NORTH', 2:'WEST', 3:'SOUTH'}\n",
    "\n",
    "dir2code = {\"EAST\":0, \"NORTH\": 1, \"WEST\":2, \"SOUTH\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.Sequential([tf.keras.layers.Dense(100,activation=\"relu\",input_shape=(WIDTH*HEIGHT,)),\n",
    "                                 tf.keras.layers.Dense(100,activation=\"relu\"),\n",
    "                                 tf.keras.layers.Dense(100,activation=\"relu\"),\n",
    "                                 tf.keras.layers.Dense(act_shape)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_func(model,obs,act):\n",
    "    return tf.reduce_sum(model(obs) * tf.one_hot(act,depth=act_shape), axis=1)\n",
    "\n",
    "def Q1_func(model,next_obs,rew,done):\n",
    "    gamma = 0.99\n",
    "    return gamma*tf.reduce_max(model(next_obs),axis=1)*(1.0-done) + rew\n",
    "\n",
    "#@tf.function\n",
    "def train_then_absTD(model,target,obs,act,rew,next_obs,done,weights):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_weights)\n",
    "        Q = Q_func(model,obs,act)\n",
    "        yQ1_r = Q1_func(target,next_obs,rew,done)\n",
    "        TD_square = tf.square(Q - yQ1_r)\n",
    "        weighted_loss = tf.reduce_mean(TD_square * weights)\n",
    "\n",
    "    grad = tape.gradient(weighted_loss,model.trainable_weights)\n",
    "    opt.apply_gradients(zip(grad,model.trainable_weights))\n",
    "\n",
    "    Qnew = Q_func(model,obs,act)\n",
    "    return tf.abs(Qnew - yQ1_r)\n",
    "\n",
    "#@tf.function\n",
    "def abs_TD(model,target,obs,act,rew,next_obs,done):\n",
    "    Q = Q_func(model,obs,act)\n",
    "    yQ1_r = Q1_func(target,next_obs,rew,done)\n",
    "    return tf.abs(Q - yQ1_r)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(index):\n",
    "    return index%WIDTH, index//WIDTH\n",
    "\n",
    "def centering(z,dz,Z):\n",
    "    z += dz\n",
    "    if z < 0:\n",
    "        z += Z\n",
    "    elif Z >= Z:\n",
    "        z -= Z\n",
    "    return z\n",
    "    \n",
    "\n",
    "def encode_board(obs,act=\"NORTH\",idx=0):\n",
    "    \"\"\"\n",
    "    Player goose is always set at the center\n",
    "    \"\"\"\n",
    "    board = np.zeros((WIDTH,HEIGHT))\n",
    "\n",
    "    if len(obs[\"geese\"][idx]) == 0:\n",
    "        return board\n",
    "        \n",
    "    x0, y0 = pos(obs[\"geese\"][idx][0])\n",
    "    dx = xc - x0\n",
    "    dy = yc - y0\n",
    "    \n",
    "    for goose in obs[\"geese\"]:\n",
    "        for g in goose[:-1]: # the last tail is safe\n",
    "            x, y = pos(g)\n",
    "            \n",
    "            x = centering(x,dx,WIDTH)\n",
    "            y = centering(y,dy,HEIGHT)\n",
    "                \n",
    "            board[x,y] = GOOSE\n",
    "            \n",
    "    for food in obs[\"food\"]:\n",
    "        x, y = pos(food)\n",
    "        \n",
    "        x = centering(x,dx,WIDTH)\n",
    "        y = centering(y,dy,HEIGHT)\n",
    "        \n",
    "        board[x,y] = FOOD\n",
    "        \n",
    "    board[xc,yc] = dir2code[act]\n",
    "\n",
    "    # Avoid Body Hit add psudo GOOSE\n",
    "    if act == \"EAST\":\n",
    "        board[WEST_idx] = GOOSE\n",
    "    elif act == \"NORTH\":\n",
    "        board[SOUTH_idx] = GOOSE\n",
    "    elif act == \"WEST\":\n",
    "        board[EAST_idx] = GOOSE\n",
    "    elif act == \"SOUTH\":\n",
    "        board[NORTH_idx] = GOOSE\n",
    "    else:\n",
    "        raise\n",
    "    \n",
    "    return board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obs_action(model,states,idx=0, train=False):\n",
    "    act = states[idx][\"action\"]\n",
    "\n",
    "    if states[idx][\"status\"] != \"ACTIVE\":\n",
    "        return None, act\n",
    "    \n",
    "    board = encode_board(states[0][\"observation\"],act=act,idx=idx)\n",
    "    \n",
    "    # e-greedy\n",
    "    if train:\n",
    "        if np.random.random() < 0.1:\n",
    "            new_act = np.random.randint(4)\n",
    "        else:\n",
    "            new_act = int(tf.math.argmax(tf.squeeze(model(board.reshape(1,-1)))))    \n",
    "    else:\n",
    "        Q = tf.squeeze(model(board.reshape(1,-1))).numpy()\n",
    "        OK = (board[AROUND] != GOOSE)\n",
    "        \n",
    "        new_act = 0\n",
    "        max_v = -99999\n",
    "        for i, (q,ok) in enumerate(zip(Q,OK)):\n",
    "            if (q > max_v) and ok:\n",
    "                new_act = i\n",
    "                max_v = q\n",
    "\n",
    "    return board, code2dir[new_act]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obs_action_greedy(states,idx=0):\n",
    "    act = states[idx][\"action\"]\n",
    "    \n",
    "    if states[idx][\"status\"] != \"ACTIVE\":\n",
    "        return None, act\n",
    "    \n",
    "    board = encode_board(states[0][\"observation\"],act=act,idx=idx)\n",
    "    \n",
    "    obs = states[0][\"observation\"]\n",
    "\n",
    "    if len(obs[\"geese\"][idx]) == 0 or len(obs[\"food\"]) == 0:\n",
    "        return board, act\n",
    "    \n",
    "    x0, y0 = pos(obs[\"geese\"][idx][0])\n",
    "    \n",
    "    min_len = WIDTH + HEIGHT\n",
    "    min_i = 0\n",
    "    NG = (board[AROUND] == GOOSE)\n",
    "    for i, food in enumerate(obs[\"food\"]):\n",
    "        x, y = pos(food)\n",
    "        \n",
    "        dx = x - x0\n",
    "        dy = y - y0\n",
    "        L = abs(dx) + abs(dy)\n",
    "        \n",
    "        if dx == 0:\n",
    "            if (dy > 0) and NG[dir2code[\"SOUTH\"]]:\n",
    "                L += 2\n",
    "            elif (dy < 0) and NG[dir2code[\"NORTH\"]]:\n",
    "                L += 2\n",
    "        if dy == 0:\n",
    "            if (dx > 0) and NG[dir2code[\"EAST\"]]:\n",
    "                L += 2\n",
    "            elif (dx < 0) and NG[dir2code[\"WEST\"]]:\n",
    "                L += 2\n",
    "            \n",
    "        if L < min_len:\n",
    "            min_len = L\n",
    "            min_i = i\n",
    "\n",
    "    food = obs[\"food\"][min_i]\n",
    "    x, y = pos(food)\n",
    "\n",
    "    if (x > x0):\n",
    "        return board, \"EAST\"\n",
    "    \n",
    "    if (x < x0):\n",
    "        return board, \"WEST\"\n",
    "    \n",
    "    if (y > y0):\n",
    "        return board, \"SOUTH\"\n",
    "    \n",
    "    if (y < y0):\n",
    "        return board, \"NORTH\"\n",
    "    \n",
    "    return board, act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_buffer(buffer_size,env_dict,alpha):\n",
    "    return cpprb.MPPrioritizedReplayBuffer(buffer_size,env_dict,alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explorer(global_rb,env_dict,is_training_done,queue):\n",
    "    local_buffer_size = int(1e+2)\n",
    "    local_rb = cpprb.ReplayBuffer(local_buffer_size+4,env_dict)\n",
    "\n",
    "    model = create_model()\n",
    "    target = tf.keras.models.clone_model(model)\n",
    "    env = make(\"hungry_geese\", debug=False)\n",
    "    \n",
    "    states = env.reset(4)\n",
    "    while not is_training_done.is_set():\n",
    "        if not queue.empty():\n",
    "            w,wt = queue.get()\n",
    "            model.set_weights(w)\n",
    "            target.set_weights(wt)\n",
    "\n",
    "        board_act = [get_obs_action(model,states,i,train=True) if i < 2 else get_obs_action_greedy(states,i)\n",
    "                     for i in range(4)]\n",
    "\n",
    "        states = env.step([a for b,a in board_act])\n",
    "\n",
    "        for i, (b, a) in enumerate(board_act):\n",
    "            if b is None:\n",
    "                continue\n",
    "\n",
    "            local_rb.add(obs=b.ravel(),\n",
    "                         act=dir2code[a],\n",
    "                         next_obs=encode_board(states[0][\"observation\"],act=a,idx=i).ravel(),\n",
    "                         rew=states[i][\"reward\"],\n",
    "                         done=(states[i][\"status\"] != \"ACTIVE\"))\n",
    "\n",
    "        if all(s[\"status\"] != \"ACTIVE\" for s in states):\n",
    "            states = env.reset(4)\n",
    "            local_rb.on_episode_end()\n",
    "\n",
    "        if local_rb.get_stored_size() >= local_buffer_size:\n",
    "            sample = local_rb.get_all_transitions()\n",
    "            global_rb.add(**sample,\n",
    "                          priorities=abs_TD(model,target,\n",
    "                                            tf.constant(sample[\"obs\"]),\n",
    "                                            tf.constant(sample[\"act\"].ravel()),\n",
    "                                            tf.constant(sample[\"rew\"].ravel()),\n",
    "                                            tf.constant(sample[\"next_obs\"]),\n",
    "                                            tf.constant(sample[\"done\"].ravel())))\n",
    "            local_rb.clear()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<unknown>, line 36)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\antho\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3418\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[0;32m\"<ipython-input-14-b257e4e9c615>\"\u001b[0m, line \u001b[0;32m1\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    get_ipython().run_cell_magic('time', '', '\\n# Training\\nn_warming = 100\\nn_train_step = int(1e+4)\\nbatch_size = 64\\n\\n# writer = tf.summary.create_file_writer(\"./logs\")\\n\\n# Replay Buffer \\nbuffer_size = 10e+5\\nenv_dict = {\"obs\": {\"shape\": (WIDTH*HEIGHT)},\\n            \"act\": {\"dtype\": int},\\n            \"next_obs\": {\"shape\": (WIDTH*HEIGHT)},\\n            \"rew\": {},\\n            \"done\": {}}\\nalpha = 0.5\\nrb = create_buffer(buffer_size, env_dict,alpha)\\n\\n# Model\\ntarget_update = 50\\n\\n\\nmodel = create_model()\\ntarget = tf.keras.models.clone_model(model)\\n\\nopt = tf.keras.optimizers.Adam()\\n\\n# Ape-X\\nexplorer_update_freq = 100\\nn_explorer = -1\\n\\n\\nprint(\"warm-up\")\\nwhile rb.get_stored_size() < n_warming:\\n\\n\\nprint(\"training\")\\n    \\nepoch = 0\\nfor i in tqdm(range(n_train_step)):        \\n    sample = rb.sample(batch_size,beta=0.4)\\n    \\n    absTD = train_then_absTD(model,target,\\n                             tf.constant(sample[\"obs\"]),\\n                             tf.constant(sample[\"act\"].ravel()),\\n                             tf.constant(sample[\"rew\"].ravel()),\\n                             tf.constant(sample[\"next_obs\"]),\\n                             tf.constant(sample[\"done\"].ravel()),\\n                             tf.constant(sample[\"weights\"].ravel()))\\n    rb.update_priorities(sample[\"indexes\"],absTD)\\n        \\n    if i % target_update == 0:\\n        target.set_weights(model.get_weights())\\n        \\n    if i % explorer_update_freq == 0:\\n        w = model.get_weights()\\n        wt = target.get_weights()\\n        for q in qs:\\n            q.put((w,wt))\\n')\n",
      "  File \u001b[0;32m\"C:\\Users\\antho\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m2382\u001b[0m, in \u001b[0;35mrun_cell_magic\u001b[0m\n    result = fn(*args, **kwargs)\n",
      "  File \u001b[0;32m\"<decorator-gen-55>\"\u001b[0m, line \u001b[0;32m2\u001b[0m, in \u001b[0;35mtime\u001b[0m\n",
      "  File \u001b[0;32m\"C:\\Users\\antho\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\magic.py\"\u001b[0m, line \u001b[0;32m187\u001b[0m, in \u001b[0;35m<lambda>\u001b[0m\n    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \u001b[0;32m\"C:\\Users\\antho\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\magics\\execution.py\"\u001b[0m, line \u001b[0;32m1277\u001b[0m, in \u001b[0;35mtime\u001b[0m\n    expr_ast = self.shell.compile.ast_parse(expr)\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\antho\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\compilerop.py\"\u001b[1;36m, line \u001b[1;32m101\u001b[1;36m, in \u001b[1;35mast_parse\u001b[1;36m\u001b[0m\n\u001b[1;33m    return compile(source, filename, symbol, self.flags | PyCF_ONLY_AST, 1)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"<unknown>\"\u001b[1;36m, line \u001b[1;32m36\u001b[0m\n\u001b[1;33m    print(\"training\")\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Training\n",
    "n_warming = 100\n",
    "n_train_step = int(1e+4)\n",
    "batch_size = 64\n",
    "\n",
    "# writer = tf.summary.create_file_writer(\"./logs\")\n",
    "\n",
    "# Replay Buffer \n",
    "buffer_size = 10e+5\n",
    "env_dict = {\"obs\": {\"shape\": (WIDTH*HEIGHT)},\n",
    "            \"act\": {\"dtype\": int},\n",
    "            \"next_obs\": {\"shape\": (WIDTH*HEIGHT)},\n",
    "            \"rew\": {},\n",
    "            \"done\": {}}\n",
    "alpha = 0.5\n",
    "rb = create_buffer(buffer_size, env_dict,alpha)\n",
    "\n",
    "# Model\n",
    "target_update = 50\n",
    "\n",
    "\n",
    "model = create_model()\n",
    "target = tf.keras.models.clone_model(model)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Ape-X\n",
    "explorer_update_freq = 100\n",
    "n_explorer = -1\n",
    "\n",
    "\n",
    "print(\"warm-up\")\n",
    "while rb.get_stored_size() < n_warming:\n",
    "\n",
    "\n",
    "print(\"training\")\n",
    "    \n",
    "epoch = 0\n",
    "for i in tqdm(range(n_train_step)):        \n",
    "    sample = rb.sample(batch_size,beta=0.4)\n",
    "    \n",
    "    absTD = train_then_absTD(model,target,\n",
    "                             tf.constant(sample[\"obs\"]),\n",
    "                             tf.constant(sample[\"act\"].ravel()),\n",
    "                             tf.constant(sample[\"rew\"].ravel()),\n",
    "                             tf.constant(sample[\"next_obs\"]),\n",
    "                             tf.constant(sample[\"done\"].ravel()),\n",
    "                             tf.constant(sample[\"weights\"].ravel()))\n",
    "    rb.update_priorities(sample[\"indexes\"],absTD)\n",
    "        \n",
    "    if i % target_update == 0:\n",
    "        target.set_weights(model.get_weights())\n",
    "        \n",
    "    if i % explorer_update_freq == 0:\n",
    "        w = model.get_weights()\n",
    "        wt = target.get_weights()\n",
    "        for q in qs:\n",
    "            q.put((w,wt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p sub\n",
    "model.save(\"sub/model\")\n",
    "\n",
    "for p in ps:\n",
    "    p.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
